{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a3ebaa",
   "metadata": {},
   "source": [
    "# Autoencoder training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2a52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "from nn import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7efdb8",
   "metadata": {},
   "source": [
    "Load digits dataset from sklearn and split data into training and testing sets using train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23c2e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (1257, 64)\n",
      "Testing data (540, 64)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "digits = load_digits()\n",
    "X_train, X_val, y_train, y_val = train_test_split(digits[\"data\"], digits[\"target\"],\n",
    "                                                 test_size = 0.3, \n",
    "                                                 random_state = 1)\n",
    "print(\"Training data \" + str(X_train.shape))\n",
    "print(\"Testing data \" + str(X_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60663df9",
   "metadata": {},
   "source": [
    "## 3-fold cross validation for tuning of neural network layers\n",
    "Split the training set into three additional batches for hypertuning of parameters.\n",
    "Additional parameters to test:\n",
    "* number of layers (2 vs 4)\n",
    "* batch size (100 vs 200)\n",
    "* epochs (100 vs 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ec6af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Layers': 2, 'Batches': 100, 'Epochs': 100},\n",
       " {'Layers': 2, 'Batches': 100, 'Epochs': 500},\n",
       " {'Layers': 2, 'Batches': 200, 'Epochs': 100},\n",
       " {'Layers': 2, 'Batches': 200, 'Epochs': 500},\n",
       " {'Layers': 4, 'Batches': 100, 'Epochs': 100},\n",
       " {'Layers': 4, 'Batches': 100, 'Epochs': 500},\n",
       " {'Layers': 4, 'Batches': 200, 'Epochs': 100},\n",
       " {'Layers': 4, 'Batches': 200, 'Epochs': 500}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cv = 3 # number of cross validation folds\n",
    "# Create dataframe to store training results\n",
    "cols = [\"idx\", \"train_error\", \"val_error\",]\n",
    "tune_res = pd.DataFrame(columns = cols)\n",
    "\n",
    "# Split the training set into three folds\n",
    "kf = KFold(n_splits = num_cv)\n",
    "\n",
    "# Define the parameters\n",
    "num_layers = {2: [{\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "                  {\"input_dim\": 16, \"output_dim\": 64, \"activation\": \"relu\"}],\n",
    "              4: [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'},\n",
    "                  {'input_dim': 32, 'output_dim': 16, 'activation': 'relu'},\n",
    "                  {'input_dim': 16, 'output_dim': 32, 'activation': 'relu'},  \n",
    "                  {'input_dim': 32, 'output_dim': 64, 'activation': 'relu'}]}\n",
    "num_batches = {100: 100, \n",
    "               200: 200}\n",
    "num_epochs = {100: 100,\n",
    "              500: 500}\n",
    "\n",
    "# Create a grid of parameter iterations\n",
    "d = {'Layers':[2, 4], \n",
    "     'Batches':[100, 200],\n",
    "     'Epochs':[100, 500]}\n",
    "param_grid = [dict(zip(d, v)) for v in product(*d.values())]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11bea4d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameter set 0\n",
      "Layers = 2, Batches = 100, Epochs = 100\n",
      "Forward:\n",
      "Layer index: 1\n",
      "Shape _W :(16, 64)\n",
      "Shape _b :(16, 1)\n",
      "Shape _A_prev: (53, 64)\n",
      "Shape Z_curr :(53, 16)\n",
      "Shape A_curr: (53, 16)\n",
      "Layer index: 2\n",
      "Shape _W :(64, 16)\n",
      "Shape _b :(64, 1)\n",
      "Shape _A_prev: (53, 16)\n",
      "Shape Z_curr :(53, 64)\n",
      "Shape A_curr: (53, 64)\n",
      "Back prop\n",
      "_MSE_BP dA shape: (53, 64)\n",
      "Layer index: 2\n",
      "Shape _W_curr :(64, 16)\n",
      "Shape _b_curr :(64, 1)\n",
      "Shape _Z_curr: (53, 64)\n",
      "Shape _A_prev:(53, 16)\n",
      "Shape _dA: (53, 64)\n",
      "Shape bp: (53, 64)\n",
      "Layer index: 1\n",
      "Shape _W_curr :(16, 64)\n",
      "Shape _b_curr :(16, 1)\n",
      "Shape _Z_curr: (53, 16)\n",
      "Shape _A_prev:(53, 64)\n",
      "Shape _dA: (53, 16)\n",
      "Shape bp: (53, 16)\n",
      "Layer index: 1\n",
      "Shape _W :(16, 64)\n",
      "Shape _b :(16, 1)\n",
      "Shape _A_prev: (419,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (419,) and (64,16) not aligned: 419 (dim 0) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m net \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNeuralNetwork(nn_arch \u001b[38;5;241m=\u001b[39m num_layers[l],\n\u001b[1;32m     20\u001b[0m                        batch_size \u001b[38;5;241m=\u001b[39m num_batches[b],\n\u001b[1;32m     21\u001b[0m                        epochs \u001b[38;5;241m=\u001b[39m num_epochs[e],\n\u001b[1;32m     22\u001b[0m                        lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m,\n\u001b[1;32m     23\u001b[0m                        seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     24\u001b[0m                        loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Get fold-specific training and testing data\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                               \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate average loss\u001b[39;00m\n\u001b[1;32m     30\u001b[0m fold_train_loss\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(train_loss))\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/BMI203/Github/FINAL-NN/nn/nn.py:382\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    379\u001b[0m per_epoch_loss_train\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(batch_train_loss))\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Compute validation loss\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_func\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    384\u001b[0m     batch_val_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mean_squared_error(y_val, pred))\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/BMI203/Github/FINAL-NN/nn/nn.py:406\u001b[0m, in \u001b[0;36mNeuralNetwork.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03mThis function returns the prediction of the neural network.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m        Prediction from the model.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# StraightFORWARD (hah pun intended)\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m y_hat, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/BMI203/Github/FINAL-NN/nn/nn.py:159\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape _A_prev: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(A_prev\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Run _single_forward\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m A_curr, Z_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_curr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_W_curr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mb_curr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_b_curr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_activation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape Z_curr :\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(Z_curr\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape A_curr: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(A_curr\u001b[38;5;241m.\u001b[39mshape))\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/BMI203/Github/FINAL-NN/nn/nn.py:112\u001b[0m, in \u001b[0;36mNeuralNetwork._single_forward\u001b[0;34m(self, W_curr, b_curr, A_prev, activation)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mThis method is used for a single forward pass on a single layer.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m        Current layer linear transformed matrix.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Apply linear transformation\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m Z_curr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_curr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b_curr\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Apply the activation function depending on activation type\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m activation\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (419,) and (64,16) not aligned: 419 (dim 0) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Iterate through the parameter grid\n",
    "for idx, _ in enumerate(param_grid):\n",
    "    # Get current parameters\n",
    "    l = param_grid[idx]['Layers']\n",
    "    b = param_grid[idx]['Batches']\n",
    "    e = param_grid[idx]['Epochs']\n",
    "    print(\"Testing parameter set \" + str(idx))\n",
    "    print(\"Layers = \" + str(l) + \", Batches = \" + str(b) + \", Epochs = \" + str(e))\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_val_loss = []\n",
    "    \n",
    "    tune_res.loc[idx, \"idx\"] = idx\n",
    "    # Iterate through the CV folds\n",
    "    for cv_fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "        # print(\"Testing fold #\" + str(cv_fold + 1))\n",
    "        \n",
    "        # Initialize neural network with current parameters\n",
    "        net = nn.NeuralNetwork(nn_arch = num_layers[l],\n",
    "                               batch_size = num_batches[b],\n",
    "                               epochs = num_epochs[e],\n",
    "                               lr = 0.0001,\n",
    "                               seed = 1,\n",
    "                               loss_function = \"mse\")\n",
    "        # Get fold-specific training and testing data\n",
    "        train_loss, val_loss = net.fit(X_train[train_idx], X_train[train_idx],\n",
    "                                       X_train[val_idx], X_train[val_idx])\n",
    "        \n",
    "        # Calculate average loss\n",
    "        fold_train_loss.append(np.mean(train_loss))\n",
    "        fold_val_loss.append(np.mean(val_loss))\n",
    "    \n",
    "    tune_res.loc[idx, \"train_error\"] = np.mean(fold_train_loss)\n",
    "    tune_res.loc[idx, \"val_error\"] = np.mean(fold_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f7e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results of tuning\n",
    "tune_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3906876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters of the tuning with the lowest validation error\n",
    "opt_idx = tune_res[\"val_error\"].astype(float).idxmin()\n",
    "# Print parameters\n",
    "print(\"Lowest validation with the following parameters:\")\n",
    "print(\"Layers: \" + str(param_grid[opt_idx]['Layers']))\n",
    "print(\"Batches: \" + str(param_grid[opt_idx]['Batches']))\n",
    "print(\"Epochs: \" + str(param_grid[opt_idx]['Epochs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215864a4",
   "metadata": {},
   "source": [
    "# Run model with the tuned parameters\n",
    "Given the outputs of the tuning, run with the \"optimal parameters\", training on the full training dataset and validating of the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f068fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = nn.NeuralNetwork(nn_arch = num_layers[param_grid[opt_idx]['Layers']],\n",
    "                       batch_size = num_batches[param_grid[opt_idx]['Batches']],\n",
    "                       epochs = num_epochs[param_grid[opt_idx]['Epochs']],\n",
    "                       lr = 0.0001,\n",
    "                       seed = 1,\n",
    "                       loss_function = \"mse\")\n",
    "\n",
    "train_loss, val_loss = net.fit(X_train, X_train, X_val, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafdf9b",
   "metadata": {},
   "source": [
    "Plot the training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131d65f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "fig.suptitle('Loss History')\n",
    "axs[0].plot(np.arange(len(train_loss)), train_loss)\n",
    "axs[0].set_title('Training Loss')\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[1].plot(np.arange(len(val_loss)), val_loss)\n",
    "axs[1].set_title('Validation Loss')\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel('Loss')\n",
    "fig.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2074b",
   "metadata": {},
   "source": [
    "Pick out one a recreation example that looked decent  (ツ)_/¯ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3db484",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net.predict(X_val)\n",
    "pred_err = mean_squared_error(X_val, pred)\n",
    "print(\"Average prediction error: \" + str(round(pred_err, 4)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
    "ax[0].matshow(X_val[3].reshape((8, 8)))\n",
    "ax[0].set_title(\"Actual\")\n",
    "ax[1].matshow(pred[3].reshape((8, 8)))\n",
    "ax[1].set_title(\"Predicted\")\n",
    "fig.tight_layout()\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
