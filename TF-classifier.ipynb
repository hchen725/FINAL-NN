{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e950b8d",
   "metadata": {},
   "source": [
    "# Transcription factor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb494b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "from nn import nn, io, preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b83361",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debde312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Rap1 positives: 137\n",
      "Length of Yeast negatives: 3163\n"
     ]
    }
   ],
   "source": [
    "rap1 = io.read_text_file(\"./data/rap1-lieb-positives.txt\")\n",
    "yeast = io.read_fasta_file(\"./data/yeast-upstream-1k-negative.fa\")\n",
    "\n",
    "print(\"Length of Rap1 positives: \" + str(len(rap1)))\n",
    "print(\"Length of Yeast negatives: \" + str(len(yeast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cc89b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of positives: 137\n",
      "Length of negatives: 183297\n",
      "Total sequences: 183434\n"
     ]
    }
   ],
   "source": [
    "pos_seq = rap1\n",
    "\n",
    "# Break up yeast_neg into sizes match the length of rap1 sequences\n",
    "seq_len = len(rap1[0])\n",
    "neg_seq = []\n",
    "\n",
    "for seq in yeast:\n",
    "    seq_sub = [seq[i:i+seq_len] for i in range(0, len(seq), seq_len)]\n",
    "    # Keep only sequences that are exactly rap length long\n",
    "    seq_sub = [x for x in seq_sub if len(x) == seq_len]\n",
    "    neg_seq += seq_sub\n",
    "\n",
    "# Combine all sequences and get labes\n",
    "seqs = pos_seq + neg_seq\n",
    "labels = [True] * len(pos_seq) + [False] * len(neg_seq)\n",
    "\n",
    "print(\"Length of positives: \" + str(len(pos_seq)))\n",
    "print(\"Length of negatives: \" + str(len(neg_seq)))\n",
    "print(\"Total sequences: \" + str(len(pos_seq) + len(neg_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda8e9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of positives: 183297\n",
      "Length of negatives: 183297\n",
      "Total sequences: 366594\n"
     ]
    }
   ],
   "source": [
    "# Up sample the positive class\n",
    "seqs2, labels2 = preprocess.sample_seqs(seqs, labels)\n",
    "print(\"Length of positives: \" + str(sum(labels2)))\n",
    "print(\"Length of negatives: \" + str(len(seqs2) - sum(labels2)))\n",
    "print(\"Total sequences: \" + str(len(seqs2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc62ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (256615, 68)\n",
      "Testing data (109979, 68)\n"
     ]
    }
   ],
   "source": [
    "# Encode sequences and create a training and testing split\n",
    "X = preprocess.one_hot_encode_seqs(seqs2)\n",
    "y = np.array(labels2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "print(\"Training data \" + str(X_train.shape))\n",
    "print(\"Testing data \" + str(X_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d4c96",
   "metadata": {},
   "source": [
    "For this dataset, I chose to upsample the positive class to match the number of observations in the negative class. The primary reason for this is because neural networks tend to do better with a larger number of observations and having 274 total observations would not yield a productive neural network function. Also "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c90ca",
   "metadata": {},
   "source": [
    "# 3-fold cross validation of tuning of neural network layers\n",
    "Parameters to test:\n",
    "* Number of batches: 100 vs 100000\n",
    "* Number of epochs: 10 vs 500\n",
    "* Learning rate: 0.01 vs 0.0001\n",
    "\n",
    "For the layers of the neural network, since we are trying to reduce the data from it's 68 features to a classification of True vs False, I chose to use a three layer neural network, halving the number of parameters till I arbitrarily felt like I didn't want to anymore. Since we are trying to perform a binary classification, a sigmoid activation seems most appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e647ebaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'batches': 100, 'epochs': 100, 'lr': 0.01},\n",
       " {'batches': 100, 'epochs': 100, 'lr': 0.0001},\n",
       " {'batches': 100, 'epochs': 500, 'lr': 0.01},\n",
       " {'batches': 100, 'epochs': 500, 'lr': 0.0001},\n",
       " {'batches': 100000, 'epochs': 100, 'lr': 0.01},\n",
       " {'batches': 100000, 'epochs': 100, 'lr': 0.0001},\n",
       " {'batches': 100000, 'epochs': 500, 'lr': 0.01},\n",
       " {'batches': 100000, 'epochs': 500, 'lr': 0.0001}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cv = 3 # number of cross validation folds\n",
    "# Create dataframe to store training results\n",
    "cols = [\"idx\", \"train_error\", \"val_error\",]\n",
    "tune_res = pd.DataFrame(columns = cols)\n",
    "\n",
    "# Split the training set into three folds\n",
    "kf = KFold(n_splits = num_cv)\n",
    "\n",
    "#Define the parameters\n",
    "layers = [{\"input_dim\" : 68, \"output_dim\" : 34, \"activation\" : \"sigmoid\"},\n",
    "          {\"input_dim\" : 34, \"output_dim\" : 17, \"activation\" : \"sigmoid\"},\n",
    "          {\"input_dim\" : 17, \"output_dim\" : 8, \"activation\" : \"sigmoid\"},\n",
    "          {\"input_dim\" : 8, \"output_dim\" : 1, \"activation\" : \"sigmoid\"}]\n",
    "num_batches = {100: 100, \n",
    "               100000: 100000}\n",
    "num_epochs = {100: 100,\n",
    "              500: 500}\n",
    "num_lr = {0.01: 0.01,\n",
    "          0.0001: 0.0001}\n",
    "\n",
    "# Create a grid of parameter iterations\n",
    "d = {'batches':[100, 100000], \n",
    "     'epochs':[100, 500],\n",
    "     'lr':[0.01, 0.0001]}\n",
    "param_grid = [dict(zip(d, v)) for v in product(*d.values())]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fed80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameter set 0\n",
      "Batches = 100, Epochs = 100, lr = 0.01\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the parameter grid\n",
    "for idx, _ in enumerate(param_grid):\n",
    "    # Get current parameters\n",
    "    b = param_grid[idx]['batches']\n",
    "    e = param_grid[idx]['epochs']\n",
    "    lr = param_grid[idx]['lr']\n",
    "    print(\"Testing parameter set \" + str(idx))\n",
    "    print(\"Batches = \" + str(b) + \", Epochs = \" + str(e) + \", lr = \" + str(lr))\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_val_loss = []\n",
    "    \n",
    "    tune_res.loc[idx, \"idx\"] = idx\n",
    "    for cv_fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "        \n",
    "        net = nn.NeuralNetwork(nn_arch = layers,\n",
    "                               batch_size = num_batches[b],\n",
    "                               epochs = num_epochs[e],\n",
    "                               lr = num_lr[lr],\n",
    "                               seed = 1,\n",
    "                               loss_function = \"bce\",\n",
    "                               verbose = False)\n",
    "        # Get fold-specific training and testing data\n",
    "        train_loss, val_loss = net.fit(X_train[train_idx], y_train[train_idx],\n",
    "                                       X_train[val_idx], y_train[val_idx])\n",
    "        \n",
    "        # Calculate average loss\n",
    "        fold_train_loss.append(np.mean(train_loss))\n",
    "        fold_val_loss.append(np.mean(val_loss))\n",
    "    \n",
    "    tune_res.loc[idx, \"train_error\"] = np.mean(fold_train_loss)\n",
    "    tune_res.loc[idx, \"val_error\"] = np.mean(fold_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69886048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of tuning\n",
    "tune_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223487e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters of the tuning with the lowest validation error\n",
    "opt_idx = tune_res[\"val_error\"].astype(float).idxmin()\n",
    "# Print parameters\n",
    "print(\"Lowest validation with the following parameters:\")\n",
    "print(\"Batches: \" + str(param_grid[opt_idx]['batches']))\n",
    "print(\"Epochs: \" + str(param_grid[opt_idx]['epochs']))\n",
    "print(\"Learning rate: \" + str(param_grid[opt_idx]['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242e223",
   "metadata": {},
   "source": [
    "## Run model with tuned parameters\n",
    "Given the outputs of the tuning, run with the \"optimal parameters\", training on the full dataset and validating the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.NeuralNetwork(nn_arch = layers,\n",
    "                       batch_size = num_batches[param_grid[opt_idx]['batches']],\n",
    "                       epochs = num_epochs[param_grid[opt_idx]['epochs']],\n",
    "                       lr = num_lr[param_grid[opt_idx]['lr']],\n",
    "                       seed = 1,\n",
    "                       loss_function = \"bce\",\n",
    "                       verbose = False)\n",
    "\n",
    "train_loss, val_loss = net.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd1b01",
   "metadata": {},
   "source": [
    "Plot the training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "fig.suptitle('Loss History')\n",
    "axs[0].plot(np.arange(len(train_loss)), train_loss)\n",
    "axs[0].set_title('Training Loss')\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[1].plot(np.arange(len(val_loss)), val_loss)\n",
    "axs[1].set_title('Validation Loss')\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel('Loss')\n",
    "fig.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf536c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
